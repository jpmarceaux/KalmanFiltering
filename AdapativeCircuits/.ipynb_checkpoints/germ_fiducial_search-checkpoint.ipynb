{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d8b0832f-2941-4fc4-8fd1-5c8375840e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygsti\n",
    "from pygsti.modelpacks import smq1Q_XYI as std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "36d9ab3c-ff69-4b92-93d0-c79b256c4218",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygsti.forwardsims import SimpleMatrixForwardSimulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5c2c10e9-e3b2-4eb1-84f5-0292f9293f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "tm = std.target_model('H+S')\n",
    "tfs1 = MatrixForwardSimulator(tm)\n",
    "dgerm = tfs1.dproduct(germs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4147c2e4-08b9-4ec1-9a65-0053083925f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLES = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d90c1084-f29c-48ad-92b6-72baa2e963be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the FOGI model\n",
    "mdl_datagen = std.target_model('H+s')\n",
    "basis1q = pygsti.baseobjs.Basis.cast('pp', 4)\n",
    "gauge_basis = pygsti.baseobjs.CompleteElementaryErrorgenBasis(\n",
    "                        basis1q, mdl_datagen.state_space, elementary_errorgen_types='HS')\n",
    "mdl_datagen.setup_fogi(gauge_basis, None, None, reparameterize=True,\n",
    "                     dependent_fogi_action='drop', include_spam=True)\n",
    "target_model = mdl_datagen.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4558fb9f-3c36-41af-8d13-86547989267a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfs2 = SimpleMatrixForwardSimulator(target_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c4e0a290-ff64-401a-aaeb-f7dc553dacd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "germs = std.germs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "135f5e65-1855-4f75-96d3-c5031e97917f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slice(18, 24, None)\n",
      "(16, 6)\n",
      "(16, 18) (16, 16) (16, 6) (16, 6)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'slice' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[101], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtfs2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdproduct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgerms\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[98], line 320\u001b[0m, in \u001b[0;36mSimpleMatrixForwardSimulator.dproduct\u001b[0;34m(self, circuit, flat, wrt_filter)\u001b[0m\n\u001b[1;32m    318\u001b[0m         LRproduct \u001b[38;5;241m=\u001b[39m _np\u001b[38;5;241m.\u001b[39mkron(leftProds[i], rightProdsT[N \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m i])  \u001b[38;5;66;03m# (dim**2, dim**2)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;28mprint\u001b[39m(flattened_dprod\u001b[38;5;241m.\u001b[39mshape, LRproduct\u001b[38;5;241m.\u001b[39mshape, dop_dopLabel\u001b[38;5;241m.\u001b[39mshape, _np\u001b[38;5;241m.\u001b[39mdot(LRproduct, dop_dopLabel)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 320\u001b[0m         flattened_dprod[\u001b[43mgpindices\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m _np\u001b[38;5;241m.\u001b[39mdot(LRproduct, dop_dopLabel)\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;66;03m#_fas(flattened_dprod, [None, gpindices], \u001b[39;00m\n\u001b[1;32m    322\u001b[0m         \u001b[38;5;66;03m#     _np.dot(LRproduct, dop_dopLabel), add=True)  # (dim**2, n_params[opLabel])\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flat:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'slice' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "tfs2.dproduct(germs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e79a3f8c-8e40-4b72-94e8-0dd1baf328db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections as _collections\n",
    "import time as _time\n",
    "import warnings as _warnings\n",
    "\n",
    "import numpy as _np\n",
    "import numpy.linalg as _nla\n",
    "\n",
    "from pygsti.forwardsims.distforwardsim import DistributableForwardSimulator as _DistributableForwardSimulator\n",
    "from pygsti.forwardsims.forwardsim import ForwardSimulator as _ForwardSimulator\n",
    "from pygsti.forwardsims.forwardsim import _bytes_for_array_types\n",
    "from pygsti.layouts.evaltree import EvalTree as _EvalTree\n",
    "from pygsti.layouts.matrixlayout import MatrixCOPALayout as _MatrixCOPALayout\n",
    "from pygsti.baseobjs.profiler import DummyProfiler as _DummyProfiler\n",
    "from pygsti.baseobjs.resourceallocation import ResourceAllocation as _ResourceAllocation\n",
    "from pygsti.baseobjs.verbosityprinter import VerbosityPrinter as _VerbosityPrinter\n",
    "from pygsti.tools import mpitools as _mpit\n",
    "from pygsti.tools import sharedmemtools as _smt\n",
    "from pygsti.tools import slicetools as _slct\n",
    "from pygsti.tools.matrixtools import _fas\n",
    "\n",
    "_dummy_profiler = _DummyProfiler()\n",
    "\n",
    "# Smallness tolerances, used internally for conditional scaling required\n",
    "# to control bulk products, their gradients, and their Hessians.\n",
    "_PSMALL = 1e-100\n",
    "_DSMALL = 1e-100\n",
    "_HSMALL = 1e-100\n",
    "\n",
    "\n",
    "class SimpleMatrixForwardSimulator(_ForwardSimulator):\n",
    "    \"\"\"\n",
    "    A forward simulator that uses matrix-matrix products to compute circuit outcome probabilities.\n",
    "    This is \"simple\" in that it adds a minimal implementation to its :class:`ForwardSimulator`\n",
    "    base class.  Because of this, it lacks some of the efficiency of a :class:`MatrixForwardSimulator`\n",
    "    object, and is mainly useful as a reference implementation and check for other simulators.\n",
    "    \"\"\"\n",
    "    # NOTE: It is currently not a *distributed* forward simulator, but after the addition of\n",
    "    # the `as_layout` method to distributed atoms, this class could instead derive from\n",
    "    # DistributableForwardSimulator and (I think) not need any more implementation.\n",
    "    # If this is done, then MatrixForwardSimulator wouldn't need to separately subclass DistributableForwardSimulator\n",
    "\n",
    "    def product(self, circuit, scale=False):\n",
    "        \"\"\"\n",
    "        Compute the product of a specified sequence of operation labels.\n",
    "        Note: LinearOperator matrices are multiplied in the reversed order of the tuple. That is,\n",
    "        the first element of circuit can be thought of as the first gate operation\n",
    "        performed, which is on the far right of the product of matrices.\n",
    "        Parameters\n",
    "        ----------\n",
    "        circuit : Circuit or tuple of operation labels\n",
    "            The sequence of operation labels.\n",
    "        scale : bool, optional\n",
    "            When True, return a scaling factor (see below).\n",
    "        Returns\n",
    "        -------\n",
    "        product : numpy array\n",
    "            The product or scaled product of the operation matrices.\n",
    "        scale : float\n",
    "            Only returned when scale == True, in which case the\n",
    "            actual product == product * scale.  The purpose of this\n",
    "            is to allow a trace or other linear operation to be done\n",
    "            prior to the scaling.\n",
    "        \"\"\"\n",
    "        if scale:\n",
    "            scaledGatesAndExps = {}\n",
    "            scale_exp = 0\n",
    "            G = _np.identity(self.model.evotype.minimal_dim(self.model.state_space))\n",
    "            for lOp in circuit:\n",
    "                if lOp not in scaledGatesAndExps:\n",
    "                    opmx = self.model.circuit_layer_operator(lOp, 'op').to_dense(on_space='minimal')\n",
    "                    ng = max(_nla.norm(opmx), 1.0)\n",
    "                    scaledGatesAndExps[lOp] = (opmx / ng, _np.log(ng))\n",
    "\n",
    "                gate, ex = scaledGatesAndExps[lOp]\n",
    "                H = _np.dot(gate, G)   # product of gates, starting with identity\n",
    "                scale_exp += ex   # scale and keep track of exponent\n",
    "                if H.max() < _PSMALL and H.min() > -_PSMALL:\n",
    "                    nG = max(_nla.norm(G), _np.exp(-scale_exp))\n",
    "                    G = _np.dot(gate, G / nG); scale_exp += _np.log(nG)  # LEXICOGRAPHICAL VS MATRIX ORDER\n",
    "                else: G = H\n",
    "\n",
    "            old_err = _np.seterr(over='ignore')\n",
    "            scale = _np.exp(scale_exp)\n",
    "            _np.seterr(**old_err)\n",
    "\n",
    "            return G, scale\n",
    "\n",
    "        else:\n",
    "            G = _np.identity(self.model.evotype.minimal_dim(self.model.state_space))\n",
    "            for lOp in circuit:\n",
    "                G = _np.dot(self.model.circuit_layer_operator(lOp, 'op').to_dense(on_space='minimal'), G)\n",
    "                # above line: LEXICOGRAPHICAL VS MATRIX ORDER\n",
    "            return G\n",
    "\n",
    "    def _rho_es_from_spam_tuples(self, rholabel, elabels):\n",
    "        # This calculator uses the convention that rho has shape (N,1)\n",
    "        rho = self.model.circuit_layer_operator(rholabel, 'prep').to_dense(on_space='minimal')[:, None]\n",
    "        Es = [_np.conjugate(_np.transpose(self.model.circuit_layer_operator(\n",
    "              elabel, 'povm').to_dense(on_space='minimal')[:, None]))\n",
    "              for elabel in elabels]  # [:, None] becuse of convention: E has shape (1,N)\n",
    "        return rho, Es\n",
    "\n",
    "    def _process_wrt_filter(self, wrt_filter, obj):\n",
    "        \"\"\" Helper function for doperation and hoperation below: pulls out pieces of\n",
    "            a wrt_filter argument relevant for a single object (gate or spam vec) \"\"\"\n",
    "\n",
    "        #Create per-gate with-respect-to parameter filters, used to\n",
    "        # select a subset of all the derivative columns, essentially taking\n",
    "        # a derivative of only a *subset* of all the gate's parameters\n",
    "\n",
    "        if isinstance(wrt_filter, slice):\n",
    "            wrt_filter = _slct.indices(wrt_filter)\n",
    "\n",
    "        if wrt_filter is not None:\n",
    "            obj_wrtFilter = []  # values = object-local param indices\n",
    "            relevant_gpindices = []  # indices into original wrt_filter'd indices\n",
    "\n",
    "            gpindices = obj.gpindices_as_array()\n",
    "\n",
    "            for ii, i in enumerate(wrt_filter):\n",
    "                if i in gpindices:\n",
    "                    relevant_gpindices.append(ii)\n",
    "                    obj_wrtFilter.append(list(gpindices).index(i))\n",
    "            relevant_gpindices = _np.array(relevant_gpindices, _np.int64)\n",
    "            if len(relevant_gpindices) == 1:\n",
    "                #Don't return a length-1 list, as this doesn't index numpy arrays\n",
    "                # like length>1 lists do... ugh.\n",
    "                relevant_gpindices = slice(relevant_gpindices[0],\n",
    "                                           relevant_gpindices[0] + 1)\n",
    "            elif len(relevant_gpindices) == 0:\n",
    "                #Don't return a length-0 list, as this doesn't index numpy arrays\n",
    "                # like length>1 lists do... ugh.\n",
    "                relevant_gpindices = slice(0, 0)  # slice that results in a zero dimension\n",
    "\n",
    "        else:\n",
    "            obj_wrtFilter = None\n",
    "            relevant_gpindices = obj.gpindices\n",
    "\n",
    "        return obj_wrtFilter, relevant_gpindices\n",
    "\n",
    "    #Vectorizing Identities. (Vectorization)\n",
    "    # Note when vectorizing op uses numpy.flatten rows are kept contiguous, so the first identity below is valid.\n",
    "    # Below we use E(i,j) to denote the elementary matrix where all entries are zero except the (i,j) entry == 1\n",
    "\n",
    "    # if vec(.) concatenates rows (which numpy.flatten does)\n",
    "    # vec( A * E(0,1) * B ) = vec( mx w/ row_i = A[i,0] * B[row1] ) = A tensor B^T * vec( E(0,1) )\n",
    "    # In general: vec( A * X * B ) = A tensor B^T * vec( X )\n",
    "\n",
    "    # if vec(.) stacks columns\n",
    "    # vec( A * E(0,1) * B ) = vec( mx w/ col_i = A[col0] * B[0,1] ) = B^T tensor A * vec( E(0,1) )\n",
    "    # In general: vec( A * X * B ) = B^T tensor A * vec( X )\n",
    "\n",
    "    def _doperation(self, op_label, flat=False, wrt_filter=None):\n",
    "        \"\"\"\n",
    "        Return the derivative of a length-1 (single-gate) sequence\n",
    "        \"\"\"\n",
    "        dim = self.model.evotype.minimal_dim(self.model.state_space)\n",
    "        gate = self.model.circuit_layer_operator(op_label, 'op')\n",
    "\n",
    "        # Allocate memory for the final result\n",
    "        num_deriv_cols = self.model.num_params if (wrt_filter is None) else len(wrt_filter)\n",
    "        #num_op_params = self.model._param_interposer.num_op_params \\\n",
    "        #    if (self.model._param_interposer is not None) else self.model.num_params\n",
    "\n",
    "        #Note: deriv_wrt_params is more accurately deriv wrt *op* params when there is an interposer\n",
    "        # d(op)/d(params) = d(op)/d(op_params) * d(op_params)/d(params)\n",
    "        if self.model._param_interposer is not None:\n",
    "            #When there is an interposer, we compute derivs wrt *all* the ops params (inefficient?),\n",
    "            # then apply interposer, then take desired wrt_filter columns:\n",
    "            assert(self.model._param_interposer.num_params == self.model.num_params)\n",
    "            num_op_params = self.model._param_interposer.num_op_params\n",
    "            deriv_wrt_op_params = _np.zeros((dim**2, num_op_params), 'd')\n",
    "            deriv_wrt_op_params[:, gate.gpindices] = gate.deriv_wrt_params()  # *don't* apply wrt filter here\n",
    "            deriv_wrt_params = _np.dot(deriv_wrt_op_params,\n",
    "                                       self.model._param_interposer.deriv_op_params_wrt_model_params())\n",
    "            # deriv_wrt_params is a derivative matrix with respect to *all* the model's parameters, so\n",
    "            # now just take requested subset:\n",
    "            flattened_dprod = deriv_wrt_params[:, wrt_filter] if (wrt_filter is not None) else deriv_wrt_params\n",
    "        else:\n",
    "            #Simpler case of no interposer: use _process_wrt_filter to \"convert\" from op params to model params\n",
    "            # (the desired op params are just some subset, given by gpindices and op_wrtFilter, of the model parameters)\n",
    "            flattened_dprod = _np.zeros((dim**2, num_deriv_cols), 'd')\n",
    "            op_wrtFilter, gpindices = self._process_wrt_filter(wrt_filter, gate)\n",
    "\n",
    "            if _slct.length(gpindices) > 0:  # works for arrays too\n",
    "                # Compute the derivative of the entire circuit with respect to the\n",
    "                # gate's parameters and fill appropriate columns of flattened_dprod.\n",
    "                #gate = self.model.operation[op_label] UNNEEDED (I think)\n",
    "                _fas(flattened_dprod, [None, gpindices],\n",
    "                     gate.deriv_wrt_params(op_wrtFilter))  # (dim**2, n_params in wrt_filter for op_label)\n",
    "\n",
    "        if flat:\n",
    "            return flattened_dprod\n",
    "        else:\n",
    "            # axes = (gate_ij, prod_row, prod_col)\n",
    "            return _np.swapaxes(flattened_dprod, 0, 1).reshape((num_deriv_cols, dim, dim))\n",
    "\n",
    "    def _hoperation(self, op_label, flat=False, wrt_filter1=None, wrt_filter2=None):\n",
    "        \"\"\"\n",
    "        Return the hessian of a length-1 (single-gate) sequence\n",
    "        \"\"\"\n",
    "        dim = self.model.evotype.minimal_dim(self.model.state_space)\n",
    "\n",
    "        gate = self.model.circuit_layer_operator(op_label, 'op')\n",
    "        op_wrtFilter1, gpindices1 = self._process_wrt_filter(wrt_filter1, gate)\n",
    "        op_wrtFilter2, gpindices2 = self._process_wrt_filter(wrt_filter2, gate)\n",
    "\n",
    "        # Allocate memory for the final result\n",
    "        num_op_params = self.model._param_interposer.num_op_params \\\n",
    "            if (self.model._param_interposer is not None) else self.model.num_params\n",
    "        num_deriv_cols1 = num_op_params if (wrt_filter1 is None) else len(wrt_filter1)\n",
    "        num_deriv_cols2 = num_op_params if (wrt_filter2 is None) else len(wrt_filter2)\n",
    "        flattened_hprod = _np.zeros((dim**2, num_deriv_cols1, num_deriv_cols2), 'd')\n",
    "\n",
    "        if _slct.length(gpindices1) > 0 and _slct.length(gpindices2) > 0:  # works for arrays too\n",
    "            # Compute the derivative of the entire circuit with respect to the\n",
    "            # gate's parameters and fill appropriate columns of flattened_dprod.\n",
    "            _fas(flattened_hprod, [None, gpindices1, gpindices2],\n",
    "                 gate.hessian_wrt_params(op_wrtFilter1, op_wrtFilter2))\n",
    "\n",
    "        #Note: deriv_wrt_params is more accurately derive wrt *op* params when there is an interposer\n",
    "        # d2(op)/d(p1)d(p2) = d2(op)/d(op_p1)d(op_p2) * d(op_p1)/d(p1) d(op_p2)/d(p2)\n",
    "        if self.model._param_interposer is not None:\n",
    "            assert(wrt_filter1 is None and wrt_filter2 is None), \"Interposers not compatible with wrt-filters yet\"\n",
    "            d_opp_wrt_p = self.model._param_interposer.deriv_op_params_wrt_model_params()\n",
    "            flattened_hprod = _np.einsum('ijk,jl,km->ilm', flattened_hprod, d_opp_wrt_p, d_opp_wrt_p)\n",
    "            num_deriv_cols1 = num_deriv_cols2 = self.model._param_interposer.num_params  # num *model* params\n",
    "\n",
    "        if flat:\n",
    "            return flattened_hprod\n",
    "        else:\n",
    "            return _np.transpose(flattened_hprod, (1, 2, 0)).reshape(\n",
    "                (num_deriv_cols1, num_deriv_cols2, dim, dim))  # axes = (gate_ij1, gateij2, prod_row, prod_col)\n",
    "\n",
    "    def dproduct(self, circuit, flat=False, wrt_filter=None):\n",
    "        \"\"\"\n",
    "        Compute the derivative of a specified sequence of operation labels.\n",
    "        Parameters\n",
    "        ----------\n",
    "        circuit : Circuit or tuple of operation labels\n",
    "            The sequence of operation labels.\n",
    "        flat : bool, optional\n",
    "            Affects the shape of the returned derivative array (see below).\n",
    "        wrt_filter : list of ints, optional\n",
    "            If not None, a list of integers specifying which gate parameters\n",
    "            to include in the derivative.  Each element is an index into an\n",
    "            array of gate parameters ordered by concatenating each gate's\n",
    "            parameters (in the order specified by the model).  This argument\n",
    "            is used internally for distributing derivative calculations across\n",
    "            multiple processors.\n",
    "        Returns\n",
    "        -------\n",
    "        deriv : numpy array\n",
    "            * if flat == False, a M x G x G array, where:\n",
    "              - M == length of the vectorized model (number of model parameters)\n",
    "              - G == the linear dimension of a operation matrix (G x G operation matrices).\n",
    "              and deriv[i,j,k] holds the derivative of the (j,k)-th entry of the product\n",
    "              with respect to the i-th model parameter.\n",
    "            * if flat == True, a N x M array, where:\n",
    "              - N == the number of entries in a single flattened gate (ordering as numpy.flatten)\n",
    "              - M == length of the vectorized model (number of model parameters)\n",
    "              and deriv[i,j] holds the derivative of the i-th entry of the flattened\n",
    "              product with respect to the j-th model parameter.\n",
    "        \"\"\"\n",
    "\n",
    "        # LEXICOGRAPHICAL VS MATRIX ORDER\n",
    "        # we do matrix multiplication in this order (easier to think about)\n",
    "        revOpLabelList = tuple(reversed(tuple(circuit)))\n",
    "        N = len(revOpLabelList)  # length of circuit\n",
    "\n",
    "        #  prod = G1 * G2 * .... * GN , a matrix                                                                                                                # noqa\n",
    "        #  dprod/d(opLabel)_ij   = sum_{L s.t. G(L) == oplabel} [ G1 ... G(L-1) dG(L)/dij G(L+1) ... GN ] , a matrix for each given (i,j)                       # noqa\n",
    "        #  vec( dprod/d(opLabel)_ij ) = sum_{L s.t. G(L) == oplabel} [ (G1 ... G(L-1)) tensor (G(L+1) ... GN)^T vec( dG(L)/dij ) ]                              # noqa\n",
    "        #                               = [ sum_{L s.t. G(L) == oplabel} [ (G1 ... G(L-1)) tensor (G(L+1) ... GN)^T ]] * vec( dG(L)/dij) )                      # noqa\n",
    "        #  if dG(L)/dij = E(i,j)                                                                                                                                # noqa\n",
    "        #                               = vec(i,j)-col of [ sum_{L s.t. G(L) == oplabel} [ (G1 ... G(L-1)) tensor (G(L+1) ... GN)^T ]]                          # noqa\n",
    "        #\n",
    "        # So for each opLabel the matrix [ sum_{L s.t. GL == oplabel} [ (G1 ... G(L-1)) tensor (G(L+1) ... GN)^T ]] has\n",
    "        # columns which correspond to the vectorized derivatives of each of the product components (i.e. prod_kl) with\n",
    "        # respect to a given gateLabel_ij.  This function returns a concatenated form of the above matrices, so that\n",
    "        # each column corresponds to a (opLabel,i,j) tuple and each row corresponds to an element of the product (els of\n",
    "        # prod.flatten()).\n",
    "        #\n",
    "        # Note: if gate G(L) is just a matrix of parameters, then dG(L)/dij = E(i,j), an elementary matrix\n",
    "\n",
    "        dim = self.model.evotype.minimal_dim(self.model.state_space)\n",
    "\n",
    "        #Cache partial products (relatively little mem required)\n",
    "        leftProds = []\n",
    "        G = _np.identity(dim); leftProds.append(G)\n",
    "        for opLabel in revOpLabelList:\n",
    "            G = _np.dot(G, self.model.circuit_layer_operator(opLabel, 'op').to_dense(on_space='minimal'))\n",
    "            leftProds.append(G)\n",
    "\n",
    "        rightProdsT = []\n",
    "        G = _np.identity(dim); rightProdsT.append(_np.transpose(G))\n",
    "        for opLabel in reversed(revOpLabelList):\n",
    "            G = _np.dot(self.model.circuit_layer_operator(opLabel, 'op').to_dense(on_space='minimal'), G)\n",
    "            rightProdsT.append(_np.transpose(G))\n",
    "\n",
    "        # Allocate memory for the final result\n",
    "        num_deriv_cols = self.model.num_params if (wrt_filter is None) else len(wrt_filter)\n",
    "        flattened_dprod = _np.zeros((dim**2, num_deriv_cols), 'd')\n",
    "\n",
    "        # For each operation label, compute the derivative of the entire circuit\n",
    "        #  with respect to only that gate's parameters and fill the appropriate\n",
    "        #  columns of flattened_dprod.\n",
    "        uniqueOpLabels = sorted(list(set(revOpLabelList)))\n",
    "        for opLabel in uniqueOpLabels:\n",
    "            gate = self.model.circuit_layer_operator(opLabel, 'op')\n",
    "            op_wrtFilter, gpindices = self._process_wrt_filter(wrt_filter, gate)\n",
    "            print(gpindices)\n",
    "            dop_dopLabel = gate.deriv_wrt_params()\n",
    "            print(dop_dopLabel.shape)\n",
    "\n",
    "            for (i, gl) in enumerate(revOpLabelList):\n",
    "                if gl != opLabel: continue  # loop over locations of opLabel\n",
    "                LRproduct = _np.kron(leftProds[i], rightProdsT[N - 1 - i])  # (dim**2, dim**2)\n",
    "                print(flattened_dprod.shape, LRproduct.shape, dop_dopLabel.shape, _np.dot(LRproduct, dop_dopLabel).shape)\n",
    "                flattened_dprod[gpindices] += _np.dot(LRproduct, dop_dopLabel)\n",
    "                #_fas(flattened_dprod, [None, gpindices], \n",
    "                #     _np.dot(LRproduct, dop_dopLabel), add=True)  # (dim**2, n_params[opLabel])\n",
    "\n",
    "        if flat:\n",
    "            return flattened_dprod\n",
    "        else:\n",
    "            # axes = (gate_ij, prod_row, prod_col)\n",
    "            return _np.swapaxes(flattened_dprod, 0, 1).reshape((num_deriv_cols, dim, dim))\n",
    "\n",
    "    def hproduct(self, circuit, flat=False, wrt_filter1=None, wrt_filter2=None):\n",
    "        \"\"\"\n",
    "        Compute the hessian of a specified sequence of operation labels.\n",
    "        Parameters\n",
    "        ----------\n",
    "        circuit : Circuit or tuple of operation labels\n",
    "            The sequence of operation labels.\n",
    "        flat : bool, optional\n",
    "            Affects the shape of the returned derivative array (see below).\n",
    "        wrt_filter1 : list of ints, optional\n",
    "            If not None, a list of integers specifying which parameters\n",
    "            to differentiate with respect to in the first (row)\n",
    "            derivative operations.  Each element is an model-parameter index.\n",
    "            This argument is used internally for distributing derivative calculations\n",
    "            across multiple processors.\n",
    "        wrt_filter2 : list of ints, optional\n",
    "            If not None, a list of integers specifying which parameters\n",
    "            to differentiate with respect to in the second (col)\n",
    "            derivative operations.  Each element is an model-parameter index.\n",
    "            This argument is used internally for distributing derivative calculations\n",
    "            across multiple processors.\n",
    "        Returns\n",
    "        -------\n",
    "        hessian : numpy array\n",
    "            * if flat == False, a  M x M x G x G numpy array, where:\n",
    "              - M == length of the vectorized model (number of model parameters)\n",
    "              - G == the linear dimension of a operation matrix (G x G operation matrices).\n",
    "              and hessian[i,j,k,l] holds the derivative of the (k,l)-th entry of the product\n",
    "              with respect to the j-th then i-th model parameters.\n",
    "            * if flat == True, a  N x M x M numpy array, where:\n",
    "              - N == the number of entries in a single flattened gate (ordered as numpy.flatten)\n",
    "              - M == length of the vectorized model (number of model parameters)\n",
    "              and hessian[i,j,k] holds the derivative of the i-th entry of the flattened\n",
    "              product with respect to the k-th then k-th model parameters.\n",
    "        \"\"\"\n",
    "\n",
    "        # LEXICOGRAPHICAL VS MATRIX ORDER\n",
    "        # we do matrix multiplication in this order (easier to think about)\n",
    "        revOpLabelList = tuple(reversed(tuple(circuit)))\n",
    "\n",
    "        #  prod = G1 * G2 * .... * GN , a matrix                                                                                                                # noqa\n",
    "        #  dprod/d(opLabel)_ij   = sum_{L s.t. GL == oplabel} [ G1 ... G(L-1) dG(L)/dij G(L+1) ... GN ] , a matrix for each given (i,j)                         # noqa\n",
    "        #  d2prod/d(opLabel1)_kl*d(opLabel2)_ij = sum_{M s.t. GM == gatelabel1} sum_{L s.t. GL == gatelabel2, M < L}                                            # noqa\n",
    "        #                                                 [ G1 ... G(M-1) dG(M)/dkl G(M+1) ... G(L-1) dG(L)/dij G(L+1) ... GN ] + {similar with L < M}          # noqa\n",
    "        #                                                 + sum{M==L} [ G1 ... G(M-1) d2G(M)/(dkl*dij) G(M+1) ... GN ]                                          # noqa\n",
    "        #                                                 a matrix for each given (i,j,k,l)                                                                     # noqa\n",
    "        #  vec( d2prod/d(opLabel1)_kl*d(opLabel2)_ij ) = sum{...} [ G1 ...  G(M-1) dG(M)/dkl G(M+1) ... G(L-1) tensor (G(L+1) ... GN)^T vec( dG(L)/dij ) ]      # noqa\n",
    "        #                                                  = sum{...} [ unvec( G1 ...  G(M-1) tensor (G(M+1) ... G(L-1))^T vec( dG(M)/dkl ) )                   # noqa\n",
    "        #                                                                tensor (G(L+1) ... GN)^T vec( dG(L)/dij ) ]                                            # noqa\n",
    "        #                                                  + sum{ L < M} [ G1 ...  G(L-1) tensor                                                                # noqa\n",
    "        #                                                       ( unvec( G(L+1) ... G(M-1) tensor (G(M+1) ... GN)^T vec( dG(M)/dkl ) ) )^T vec( dG(L)/dij ) ]   # noqa\n",
    "        #                                                  + sum{ L == M} [ G1 ...  G(M-1) tensor (G(M+1) ... GN)^T vec( d2G(M)/dkl*dji )                       # noqa\n",
    "        #\n",
    "        #  Note: ignoring L == M terms assumes that d^2 G/(dij)^2 == 0, which is true IF each operation matrix element\n",
    "        #  is at most *linear* in each of the gate parameters.  If this is not the case, need LinearOperator objects to\n",
    "        #  have a 2nd-deriv method in addition of deriv_wrt_params\n",
    "        #\n",
    "        #  Note: unvec( X ) can be done efficiently by actually computing X^T ( note (A tensor B)^T = A^T tensor B^T )\n",
    "        #  and using numpy's reshape\n",
    "\n",
    "        dim = self.model.evotype.minimal_dim(self.model.state_space)\n",
    "\n",
    "        uniqueOpLabels = sorted(list(set(revOpLabelList)))\n",
    "        used_operations = _collections.OrderedDict()\n",
    "\n",
    "        #Cache processed parameter filters for multiple uses below\n",
    "        gpindices1 = {}; gate_wrtFilters1 = {}\n",
    "        gpindices2 = {}; gate_wrtFilters2 = {}\n",
    "        for l in uniqueOpLabels:\n",
    "            used_operations[l] = self.model.circuit_layer_operator(l, 'op')\n",
    "            gate_wrtFilters1[l], gpindices1[l] = self._process_wrt_filter(wrt_filter1, used_operations[l])\n",
    "            gate_wrtFilters2[l], gpindices2[l] = self._process_wrt_filter(wrt_filter2, used_operations[l])\n",
    "\n",
    "        #Cache partial products (relatively little mem required)\n",
    "        prods = {}\n",
    "        ident = _np.identity(dim)\n",
    "        for (i, opLabel1) in enumerate(revOpLabelList):  # loop over \"starting\" gate\n",
    "            prods[(i, i - 1)] = ident  # product of no gates\n",
    "            G = ident\n",
    "            for (j, opLabel2) in enumerate(revOpLabelList[i:], start=i):  # loop over \"ending\" gate (>= starting gate)\n",
    "                G = _np.dot(G, self.model.circuit_layer_operator(opLabel2, 'op').to_dense(on_space='minimal'))\n",
    "                prods[(i, j)] = G\n",
    "        prods[(len(revOpLabelList), len(revOpLabelList) - 1)] = ident  # product of no gates\n",
    "\n",
    "        #Also Cache gate jacobians (still relatively little mem required)\n",
    "        dop_dopLabel1 = {\n",
    "            opLabel: gate.deriv_wrt_params(gate_wrtFilters1[opLabel])\n",
    "            for opLabel, gate in used_operations.items()}\n",
    "\n",
    "        if wrt_filter1 == wrt_filter2:\n",
    "            dop_dopLabel2 = dop_dopLabel1\n",
    "        else:\n",
    "            dop_dopLabel2 = {\n",
    "                opLabel: gate.deriv_wrt_params(gate_wrtFilters2[opLabel])\n",
    "                for opLabel, gate in used_operations.items()}\n",
    "\n",
    "        #Finally, cache any nonzero gate hessians (memory?)\n",
    "        hop_dopLabels = {}\n",
    "        for opLabel, gate in used_operations.items():\n",
    "            if gate.has_nonzero_hessian():\n",
    "                hop_dopLabels[opLabel] = gate.hessian_wrt_params(\n",
    "                    gate_wrtFilters1[opLabel], gate_wrtFilters2[opLabel])\n",
    "\n",
    "        # Allocate memory for the final result\n",
    "        num_deriv_cols1 = self.model.num_params if (wrt_filter1 is None) else len(wrt_filter1)\n",
    "        num_deriv_cols2 = self.model.num_params if (wrt_filter2 is None) else len(wrt_filter2)\n",
    "        flattened_d2prod = _np.zeros((dim**2, num_deriv_cols1, num_deriv_cols2), 'd')\n",
    "\n",
    "        # For each pair of gates in the string, compute the hessian of the entire\n",
    "        #  circuit with respect to only those two gates' parameters and fill\n",
    "        #  add the result to the appropriate block of flattened_d2prod.\n",
    "\n",
    "        #NOTE: if we needed to perform a hessian calculation (i.e. for l==m) then\n",
    "        # it could make sense to iterate through the self.operations.keys() as in\n",
    "        # dproduct(...) and find the labels in the string which match the current\n",
    "        # gate (so we only need to compute this gate hessian once).  But since we're\n",
    "        # assuming that the gates are at most linear in their parameters, this\n",
    "        # isn't currently needed.\n",
    "\n",
    "        N = len(revOpLabelList)\n",
    "        for m, opLabel1 in enumerate(revOpLabelList):\n",
    "            inds1 = gpindices1[opLabel1]\n",
    "            nDerivCols1 = dop_dopLabel1[opLabel1].shape[1]\n",
    "            if nDerivCols1 == 0: continue\n",
    "\n",
    "            for l, opLabel2 in enumerate(revOpLabelList):\n",
    "                inds2 = gpindices1[opLabel2]\n",
    "                #nDerivCols2 = dop_dopLabel2[opLabel2].shape[1]\n",
    "\n",
    "                # FUTURE: we could add logic that accounts for the symmetry of the Hessian, so that\n",
    "                # if gl1 and gl2 are both in opsToVectorize1 and opsToVectorize2 we only compute d2(prod)/d(gl1)d(gl2)\n",
    "                # and not d2(prod)/d(gl2)d(gl1) ...\n",
    "\n",
    "                if m < l:\n",
    "                    x0 = _np.kron(_np.transpose(prods[(0, m - 1)]), prods[(m + 1, l - 1)])  # (dim**2, dim**2)\n",
    "                    x = _np.dot(_np.transpose(dop_dopLabel1[opLabel1]), x0); xv = x.view()  # (nDerivCols1,dim**2)\n",
    "                    xv.shape = (nDerivCols1, dim, dim)  # (reshape without copying - throws error if copy is needed)\n",
    "                    y = _np.dot(_np.kron(xv, _np.transpose(prods[(l + 1, N - 1)])), dop_dopLabel2[opLabel2])\n",
    "                    # above: (nDerivCols1,dim**2,dim**2) * (dim**2,nDerivCols2) = (nDerivCols1,dim**2,nDerivCols2)\n",
    "                    flattened_d2prod[:, inds1, inds2] += _np.swapaxes(y, 0, 1)\n",
    "                    # above: dim = (dim2, nDerivCols1, nDerivCols2);\n",
    "                    # swapaxes takes (kl,vec_prod_indx,ij) => (vec_prod_indx,kl,ij)\n",
    "                elif l < m:\n",
    "                    x0 = _np.kron(_np.transpose(prods[(l + 1, m - 1)]), prods[(m + 1, N - 1)])  # (dim**2, dim**2)\n",
    "                    x = _np.dot(_np.transpose(dop_dopLabel1[opLabel1]), x0); xv = x.view()  # (nDerivCols1,dim**2)\n",
    "                    xv.shape = (nDerivCols1, dim, dim)  # (reshape without copying - throws error if copy is needed)\n",
    "                    # transposes each of the now un-vectorized dim x dim mxs corresponding to a single kl\n",
    "                    xv = _np.swapaxes(xv, 1, 2)\n",
    "                    y = _np.dot(_np.kron(prods[(0, l - 1)], xv), dop_dopLabel2[opLabel2])\n",
    "                    # above: (nDerivCols1,dim**2,dim**2) * (dim**2,nDerivCols2) = (nDerivCols1,dim**2,nDerivCols2)\n",
    "\n",
    "                    flattened_d2prod[:, inds1, inds2] += _np.swapaxes(y, 0, 1)\n",
    "                    # above: dim = (dim2, nDerivCols1, nDerivCols2);\n",
    "                    # swapaxes takes (kl,vec_prod_indx,ij) => (vec_prod_indx,kl,ij)\n",
    "\n",
    "                else:\n",
    "                    # l==m, which we *used* to assume gave no contribution since we assume all gate elements are at most\n",
    "                    # linear in the parameters\n",
    "                    assert(opLabel1 == opLabel2)\n",
    "                    if opLabel1 in hop_dopLabels:  # indicates a non-zero hessian\n",
    "                        x0 = _np.kron(_np.transpose(prods[(0, m - 1)]), prods[(m + 1, N - 1)])  # (dim**2, dim**2)\n",
    "                        # (nDerivCols1,nDerivCols2,dim**2)\n",
    "                        x = _np.dot(_np.transpose(hop_dopLabels[opLabel1], axes=(1, 2, 0)), x0); xv = x.view()\n",
    "                        xv = _np.transpose(xv, axes=(2, 0, 1))  # (dim2, nDerivCols1, nDerivCols2)\n",
    "                        flattened_d2prod[:, inds1, inds2] += xv\n",
    "\n",
    "        if flat:\n",
    "            return flattened_d2prod  # axes = (vectorized_op_el_index, model_parameter1, model_parameter2)\n",
    "        else:\n",
    "            vec_kl_size, vec_ij_size = flattened_d2prod.shape[1:3]  # == num_deriv_cols1, num_deriv_cols2\n",
    "            return _np.rollaxis(flattened_d2prod, 0, 3).reshape((vec_kl_size, vec_ij_size, dim, dim))\n",
    "            # axes = (model_parameter1, model_parameter2, model_element_row, model_element_col)\n",
    "\n",
    "    def _compute_circuit_outcome_probabilities(self, array_to_fill, circuit, outcomes, resource_alloc, time=None):\n",
    "        \"\"\"\n",
    "        Compute probabilities of a multiple \"outcomes\" for a single circuit.\n",
    "        The outcomes correspond to `circuit` sandwiched between `rholabel` (a state preparation)\n",
    "        and the multiple effect labels in `elabels`.\n",
    "        Parameters\n",
    "        ----------\n",
    "        rholabel : Label\n",
    "            The state preparation label.\n",
    "        elabels : list\n",
    "            A list of :class:`Label` objects giving the *simplified* effect labels.\n",
    "        circuit : Circuit or tuple\n",
    "            A tuple-like object of *simplified* gates (e.g. may include\n",
    "            instrument elements like 'Imyinst_0')\n",
    "        use_scaling : bool, optional\n",
    "            Whether to use a post-scaled product internally.  If False, this\n",
    "            routine will run slightly faster, but with a chance that the\n",
    "            product will overflow and the subsequent trace operation will\n",
    "            yield nan as the returned probability.\n",
    "        time : float, optional\n",
    "            The *start* time at which `circuit` is evaluated.\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "            An array of floating-point probabilities, corresponding to\n",
    "            the elements of `elabels`.\n",
    "        \"\"\"\n",
    "        use_scaling = False  # Hardcoded for now\n",
    "        assert(time is None), \"MatrixForwardSimulator cannot be used to simulate time-dependent circuits\"\n",
    "\n",
    "        expanded_circuit_outcomes = circuit.expand_instruments_and_separate_povm(self.model, outcomes)\n",
    "        outcome_to_index = {outc: i for i, outc in enumerate(outcomes)}\n",
    "        for spc, spc_outcomes in expanded_circuit_outcomes.items():  # spc is a SeparatePOVMCircuit\n",
    "            indices = [outcome_to_index[o] for o in spc_outcomes]\n",
    "            rholabel = spc.circuit_without_povm[0]\n",
    "            circuit_ops = spc.circuit_without_povm[1:]\n",
    "            rho, Es = self._rho_es_from_spam_tuples(rholabel, spc.full_effect_labels)\n",
    "            #shapes: rho = (N,1), Es = (len(elabels),N)\n",
    "\n",
    "            if use_scaling:\n",
    "                old_err = _np.seterr(over='ignore')\n",
    "                G, scale = self.product(circuit_ops, True)\n",
    "                # TODO - add a \".dense_space_type\" attribute of evotype that == either \"Hilbert\" or \"Hilbert-Schmidt\"?\n",
    "                if self.model.evotype == \"statevec\":\n",
    "                    ps = _np.real(_np.abs(_np.dot(Es, _np.dot(G, rho)) * scale)**2)\n",
    "                else:  # evotype == \"densitymx\"\n",
    "                    # probability, with scaling applied (may generate overflow, but OK)\n",
    "                    ps = _np.real(_np.dot(Es, _np.dot(G, rho)) * scale)\n",
    "                _np.seterr(**old_err)\n",
    "\n",
    "            else:  # no scaling -- faster but susceptible to overflow\n",
    "                G = self.product(circuit_ops, False)\n",
    "                if self.model.evotype == \"statevec\":\n",
    "                    ps = _np.real(_np.abs(_np.dot(Es, _np.dot(G, rho)))**2)\n",
    "                else:  # evotype == \"densitymx\"\n",
    "                    ps = _np.real(_np.dot(Es, _np.dot(G, rho)))\n",
    "            array_to_fill[indices] = ps.flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8419c90-f10a-4055-82f9-2023c8ca83ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bf7dcf-2f28-4f2d-b384-1ffe4838d1c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
